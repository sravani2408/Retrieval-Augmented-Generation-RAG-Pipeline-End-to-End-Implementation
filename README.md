This project implements a complete Retrieval-Augmented Generation (RAG) system that transforms unstructured enterprise documents into a searchable vector database and uses them to produce grounded, context-aware answers via a Large Language Model (LLM). It covers all stages of a modern RAG workflow—including data ingestion, PDF/text parsing, chunking, embeddings, vector database indexing, retrieval, and prompt augmentation—and demonstrates how to build production-quality pipelines that surface the right context for LLMs.

This project proves ability to build full-stack Generative AI solutions on top of embeddings, vector search, semantic retrieval, and pipeline architecture. It is structured for extensibility and ready for deployment in real-world enterprise environments.
